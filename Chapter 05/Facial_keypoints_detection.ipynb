{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e382085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, models, datasets\n",
    "from torchsummary import summary\n",
    "import numpy as np, pandas as pd, os, glob\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import glob\n",
    "from sklearn import cluster\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "724f6013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "241b07c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'P1_Facial_Keypoints/data/training/'\n",
    "all_img_paths = glob.glob(os.path.join(root_dir, '*.jpg'))\n",
    "data = pd.read_csv('P1_Facial_Keypoints/data/training_frames_keypoints.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82b99356",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import cv2, numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "class FacesData(Dataset):\n",
    "    def __init__(self, df):\n",
    "        super(FacesData, self).__init__()\n",
    "        self.df = df\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                              std=[0.229, 0.224, 0.225])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, ix):\n",
    "        img_path = 'P1_Facial_Keypoints/data/training/' + self.df.iloc[ix,0]\n",
    "        img = cv2.imread(img_path)/255.\n",
    "        kp = deepcopy(self.df.iloc[ix, 1:].tolist())\n",
    "        kp_x = (np.array(kp[0::2])/img.shape[1]).tolist()\n",
    "        kp_y = (np.array(kp[1::2])/img.shape[0]).tolist()\n",
    "        kp2 = kp_x + kp_y\n",
    "        kp2 = torch.tensor(kp2)\n",
    "        img = self.preprocess_input(img)\n",
    "        return img, kp2\n",
    "    \n",
    "    def preprocess_input(self, img):\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        img = torch.tensor(img).permute(2, 0, 1)\n",
    "        img = self.normalize(img).float()\n",
    "        return img.to(device)\n",
    "    \n",
    "    def load_img(self, ix):\n",
    "        img_path = 'P1_Facial_Keypoints/data/training/' + self.df.iloc[ix,0]        \n",
    "        img = cv2.imread(img_path)\n",
    "        img =cv2.cvtColor(img, cv2.COLOR_BGR2RGB)/255.\n",
    "        img = cv2.resize(img, (224,224))\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a004b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=101)\n",
    "train_dataset = FacesData(train.reset_index(drop=True))\n",
    "test_dataset = FacesData(test.reset_index(drop=True))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7fc943a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = models.vgg16(pretrained=True)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    model.avgpool = nn.Sequential( nn.Conv2d(512,512,3),\n",
    "      nn.MaxPool2d(2),\n",
    "      nn.Flatten())\n",
    "    model.classifier = nn.Sequential(\n",
    "      nn.Linear(2048, 512),\n",
    "      nn.ReLU(),\n",
    "      nn.Dropout(0.5),\n",
    "      nn.Linear(512, 136),\n",
    "      nn.Sigmoid()\n",
    "    )\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    return model.to(device), criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44e934bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sann-htet/anaconda3/envs/tensorflow/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/sann-htet/anaconda3/envs/tensorflow/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model, criterion, optimizer = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65c6661d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(img, kps, model, optimizer, criterion):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    _kps = model(img.to(device))\n",
    "    loss = criterion(_kps, kps.to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "351c2e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate_batch(img, kps, model, criterion):\n",
    "    model.eval()\n",
    "    _kps = model(img.to(device))\n",
    "    loss = criterion(_kps, kps.to(device))\n",
    "    return _kps, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd9e9a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch 1 : 50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m epoch_train_loss, epoch_test_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ix, (img,kps) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m----> 8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m train_batch(img, kps, model, optimizer, criterion)\n\u001b[1;32m      9\u001b[0m     epoch_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \n\u001b[1;32m     10\u001b[0m epoch_train_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m (ix\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m, in \u001b[0;36mtrain_batch\u001b[0;34m(img, kps, model, optimizer, criterion)\u001b[0m\n\u001b[1;32m      3\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      4\u001b[0m _kps \u001b[38;5;241m=\u001b[39m model(img\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m----> 5\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(_kps, kps\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m      6\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss, test_loss = [], []\n",
    "n_epochs = 50\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\" epoch {epoch+ 1} : 50\")\n",
    "    epoch_train_loss, epoch_test_loss = 0, 0\n",
    "    for ix, (img,kps) in enumerate(train_loader):\n",
    "        loss = train_batch(img, kps, model, optimizer, criterion)\n",
    "        epoch_train_loss += loss.item() \n",
    "    epoch_train_loss /= (ix+1)\n",
    "\n",
    "    for ix,(img,kps) in enumerate(test_loader):\n",
    "        ps, loss = validate_batch(img, kps, model, criterion)\n",
    "        epoch_test_loss += loss.item() \n",
    "    epoch_test_loss /= (ix+1)\n",
    "\n",
    "    train_loss.append(epoch_train_loss)\n",
    "    test_loss.append(epoch_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c98876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "%matplotlib inline\n",
    "\n",
    "epochs = np.arange(50)+1\n",
    "plt.plot(epochs, train_loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, test_loss, 'r', label='Test loss')\n",
    "plt.title('Training and Test loss over increasing epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbc6f94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
